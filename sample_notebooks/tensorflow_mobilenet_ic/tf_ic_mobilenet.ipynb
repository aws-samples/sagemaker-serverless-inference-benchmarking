{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install sagemaker -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a pretrained Image Classification Model\n",
    "In this example, we'll deploy a pretrained Image Classification model using SageMaker Jumpstart then benchmark the model using the SageMaker Serverless Inference Benchmarking toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "import uuid\n",
    "\n",
    "role = sagemaker.get_execution_role() # manually provide role if using non role based identity\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, model_version = \"tensorflow-ic-imagenet-mobilenet-v2-100-224-classification-4\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"js-tf-ic-mobilenet-{str(uuid.uuid1())[:5]}\"\n",
    "\n",
    "inference_instance_type = \"ml.m5.xlarge\" # used to lookup cpu inference container. No instance will be deployed\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the inference script uri. This includes all dependencies and scripts for model loading, inference handling etc.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the model uri. This includes the pre-trained model and parameters.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create the SageMaker model instance\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    model_data=model_uri,\n",
    "    entry_point=\"inference.py\",  # entry point file in source_dir and present in deploy_source_uri\n",
    "    role=role,\n",
    "    name=model_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session = sess\n",
    "model.create(instance_type=inference_instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Endpoint\n",
    "Before launching a full benchmarking job, it is a good idea to first deploy the model on a test endpoint to ensure everything is functioning as it should. Here we will deploy a temporary endpoint and test it with an example payload. Afterwards, the endpoint is deleted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temporary endpoint\n",
    "from sm_serverless_benchmarking.endpoint import ServerlessEndpoint\n",
    "endpoint = ServerlessEndpoint(model_name=model.name, memory_size=6144)\n",
    "endpoint.create_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "sample_image_path = Path(\"sample_images\")\n",
    "image_paths = list(sample_image_path.glob(\"*.JPEG\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke it with a sample payload and make sure a valid response is returned\n",
    "image_payload = image_paths[0].open(\"rb\").read()\n",
    "response = endpoint.invoke_endpoint({\"Body\": image_payload, \"ContentType\": \"application/x-image\"})\n",
    "print(len(response[\"Body\"].read().decode(\"utf8\"))) # response is a long list of probabilities so just printing the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.clean_up() # delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Benchmarking SageMaker Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sm_serverless_benchmarking.utils import convert_invoke_args_to_pkl\n",
    "from sm_serverless_benchmarking.sagemaker_runner import run_as_sagemaker_job\n",
    "\n",
    "example_invoke_args = [\n",
    "    {\"Body\": img.open(\"rb\").read(), \"ContentType\": \"application/x-image\"}\n",
    "    for img in image_paths\n",
    "]\n",
    "\n",
    "example_invoke_file = convert_invoke_args_to_pkl(example_invoke_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-serverless-inf-bench-2022-08-15-20-01-03-963\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-152804913371/sagemaker-serverless-inf-bench-2022-08-15-20-01-03-963/input/input-1/invoke_args_examples.pkl', 'LocalPath': '/opt/ml/processing/input/data', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-152804913371/sagemaker-serverless-inf-bench-2022-08-15-20-01-03-963/source/sourcedir.tar.gz', 'LocalPath': '/opt/ml/processing/input/code/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'entrypoint', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-152804913371/sagemaker-serverless-inf-bench-2022-08-15-20-01-03-963/source/runproc.sh', 'LocalPath': '/opt/ml/processing/input/entrypoint', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'benchmark_outputs', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-152804913371/sagemaker-serverless-inf-bench-2022-08-15-20-01-03-963/output/benchmark_outputs', 'LocalPath': '/opt/ml/processing/output/', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "processor = run_as_sagemaker_job(\n",
    "    role=role,\n",
    "    model_name=model.name,\n",
    "    invoke_args_examples_file=example_invoke_file\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Outputs of the benchmarking job will be uploaded to {processor.latest_job.outputs[0].destination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally run the command below to copy all of the benchmark output artifacts into the current directory. The primary report output will be under the `benchmarking_report/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive {processor.latest_job.outputs[0].destination} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Local Benchmarking Job [OPTIONAL]\n",
    "You can also run the same benchmark locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sm_serverless_benchmarking.benchmark import run_serverless_benchmarks\n",
    "report = run_serverless_benchmarks(model_name=model.name, invoke_args_examples_file=example_invoke_file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37058495916f7ab2e7db9963171426deb73c0dc04073ed3a56b3427789bc2f48"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('serverless-benchmarking')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
